import streamlit as st
from src.fastmcp.mcp_client3 import get_gemini_client
from dotenv import load_dotenv
import asyncio

load_dotenv()


@st.cache_resource
async def get_cached_client():
    return await get_gemini_client()  # ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰


async def ask_llm2(user_query: str) -> str:
    client = await get_cached_client()
    return await client.chat(user_query)


# OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
async def ask_llm(user_query: str) -> str:
    client = await get_gemini_client()
    if client is None:
        return "ì„œë²„ì— ì—°ê²°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    result = await client.chat(user_query)
    await client.cleanup()
    return result


# í˜ì´ì§€ ë ˆì´ì•„ì›ƒ
st.set_page_config(page_title="LLM Query Demo", layout="centered")

st.title("ğŸ” LLM Query Demo")
st.write("LLMì—ê²Œ ì§ˆë¬¸ì„ ë³´ë‚´ê³  ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”.")

# ì‚¬ìš©ì ì…ë ¥
query = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=120)

# ë²„íŠ¼ í´ë¦­ ì‹œ LLM í˜¸ì¶œ
# Streamlit 1.50.0ì—ì„œëŠ” ì§ì ‘ await ì‚¬ìš© ë¶ˆê°€
# asyncio.run()ë„ ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ì‚¬ìš© ë¶ˆê°€
# í•´ê²°ì±…: ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ë§Œë“¤ê³ , ìŠ¤ë ˆë“œì—ì„œ run_until_completeë¡œ async í•¨ìˆ˜ ì‹¤í–‰
# í´ë¼ì´ì–¸íŠ¸(get_client())ëŠ” ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ê³  ì¬ì‚¬ìš©
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if query.strip():
        with st.spinner("LLM ì‘ë‹µ ìƒì„± ì¤‘..."):

            def run_async_task(coro):
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                return loop.run_until_complete(coro)

            result = run_async_task(ask_llm(query))

            st.subheader("LLM ì‘ë‹µ:")
            st.write(result)
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
