"""
Gemini MCP í´ë¼ì´ì–¸íŠ¸ - MCP ì„œë²„ì™€ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸
"""

import asyncio
import json
import subprocess
import sys
import os
import platform
import threading
import queue
from typing import Dict, Any, List, Optional, cast
from contextlib import AsyncExitStack  # for managing mulple async tasks
from mcp import ClientSession, StdioServerParameters, types as mcptypes
from mcp.client.stdio import stdio_client

# import google.generativeai as genai 2025ë…„ 8ì›” 31ì¼ ì´í›„ ì¢…ë£Œ

from google import genai
from google.genai import types as genai_types
from google.genai.types import Tool, FunctionDeclaration, GenerateContentConfig
from dataclasses import dataclass


@dataclass
class MCPServer:
    """MCP ì„œë²„ ì •ë³´"""

    name: str
    process: subprocess.Popen
    tools: List[Dict[str, Any]]
    input_queue: queue.Queue
    output_queue: queue.Queue
    error_queue: queue.Queue


class GeminiMCPClient:
    def __init__(self, api_key: str, project_id: str):
        """
        Gemini MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            api_key: Gemini API í‚¤
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()

        self.genai_client = genai.Client(api_key=api_key)

    async def connect_to_server(self, server_script_path: str) -> bool:
        """
        MCP ì„œë²„ì— ì—°ê²°

        Args:
            server_path: MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ

        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        command = "python" if server_script_path.endswith(".py") else "node"

        server_params = StdioServerParameters(
            command=command, args=[server_script_path]
        )

        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(*stdio_transport)
        )
        # Send an initialization reqeust to the MCP server.
        await self.session.initialize()

        return True

    async def call_tool(
        self, tool_name: str, arguments: Dict[str, Any]
    ) -> Optional[mcptypes.CallToolResult]:
        """
        MCP ì„œë²„ì˜ ë„êµ¬ í˜¸ì¶œ

        Args:
            tool_name: ë„êµ¬ ì´ë¦„
            arguments: ë„êµ¬ ì¸ìˆ˜

        Returns:
            ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
        """
        if not self.session:
            return

        return await self.session.call_tool(name=tool_name, arguments=arguments)

    async def get_available_tools(self) -> list[mcptypes.Tool]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        if self.session:
            response = await self.session.list_tools()
            tools = response.tools
            print(f"âœ… ì„œë²„ ì—°ê²° ì„±ê³µ, tools: {[tool.name for tool in tools]}")
            return tools
        return []

    async def cleanup(self):
        """ëª¨ë“  ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬"""
        # self.stdio.close()
        # self.write.close()
        if self.exit_stack:
            await self.exit_stack.aclose()

    async def chat(self, message: str) -> str:
        """
        Geminiì™€ ì±„íŒ…í•˜ë©° í•„ìš”ì‹œ MCP ë„êµ¬ ì‚¬ìš©

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            Gemini ì‘ë‹µ
        """
        model = "gemini-2.5-flash-lite"

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        tools_info: List[Any] = await self.get_available_tools()
        tools_description = "\n".join(
            [
                f"- {tool.name} (ìŠ¤í‚¤ë§ˆ: {tool.inputSchema}): {tool.description}"
                for tool in tools_info
            ]
        )

        system_prompt = f"""
ë‹¹ì‹ ì€ MCP(Model Context Protocol) ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{tools_description}

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ë„êµ¬ê°€ í•„ìš”í•˜ë‹¤ë©´, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ì„¸ìš”:

TOOL_CALL: {{
  "tool": "ë„êµ¬ëª…",
  "arguments": {{ì¸ìˆ˜ ë”•ì…”ë„ˆë¦¬}}
}}

ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°›ì€ í›„ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
        try:
            full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì: {message}"
            response: genai_types.GenerateContentResponse = (
                self.genai_client.chats.create(model=model).send_message(
                    message=full_prompt
                )
            )

            response_text = response.text
            if response_text is None:
                return "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."

            # ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œì§€ í™•ì¸
            if "TOOL_CALL:" in response_text:
                # ë„êµ¬ í˜¸ì¶œ ë¶€ë¶„ ì¶”ì¶œ
                tool_call_start = response_text.find("TOOL_CALL:")
                tool_call_end = response_text.rfind("}") + 1
                tool_call_json = response_text[
                    tool_call_start + 10 : tool_call_end
                ].strip()

                try:
                    tool_call = json.loads(tool_call_json.replace(r"\n", ""))
                    tool_name = tool_call["tool"]
                    arguments = tool_call["arguments"]

                    print(f"ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {tool_name}/{arguments}")

                    # ë„êµ¬ ì‹¤í–‰
                    tool_result: Optional[mcptypes.CallToolResult] = (
                        await self.call_tool(tool_name, arguments)
                    )
                    if tool_result is None:
                        return "ë„êµ¬ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

                    tool_answer = cast(
                        mcptypes.TextContent, tool_result.content[0]
                    ).text

                    # ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìµœì¢… ì‘ë‹µ ìƒì„±
                    final_prompt = f"""
{system_prompt}

ì‚¬ìš©ì: {message}

ë„êµ¬ í˜¸ì¶œ ê²°ê³¼:
ë„êµ¬: {tool_name}
ê²°ê³¼: {tool_answer}
 

ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""

                    # self.genai_client.chats.create(model="gemini-1.5-turbo").send_message
                    final_response = self.genai_client.chats.create(
                        model=model
                    ).send_message(message=final_prompt)
                    if final_response.text is None:
                        return "ìµœì¢… ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."
                    return final_response.text

                except json.JSONDecodeError:
                    return response_text

            return response_text

        except Exception as e:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


async def main():
    from dotenv import load_dotenv

    load_dotenv()

    api_key = os.getenv("GOOGLE_API_KEY")
    project_id = os.getenv("PROJECT_ID")
    if api_key is None or project_id is None:
        print("GENAI_API_KEY ë˜ëŠ” GENAI_PROJECT_ID í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    client = None
    try:
        client = GeminiMCPClient(api_key=api_key, project_id=project_id)
        await client.connect_to_server(
            r"D:\works\ai-projects\agentic-mcp-proj\gemini_cli_mcp\src\fastmcp\mcp_server3.py"
        )
        print(f"ê²°ê³¼ = {await client.chat("1+3ì€ ì–¼ë§ˆì¸ê°€ìš”?")}")
    finally:
        if client:
            await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
